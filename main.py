from serpapi import GoogleSearch
import os
import time
import re
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
API_KEY = os.getenv("API_KEY")

# Googleæ¤œç´¢ç”¨ã®åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
BASE_PARAMS = {
    "api_key": API_KEY,
    "engine": "google",
    "q": "site:https://hrmos.co/pages/*/jobs",
    "location": "Japan",
    "google_domain": "google.co.jp",
    "gl": "jp",
    "hl": "ja",
    "num": 10,  # 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®çµæœæ•°
}

# å–å¾—ã—ãŸURLã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆï¼ˆé‡è¤‡ã‚’é™¤ããŸã‚ã«setã‚’ä½¿ç”¨ï¼‰
job_urls = set()

# URLæ•´å½¢é–¢æ•°ï¼ˆä¸è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šé™¤ï¼‰
def clean_url(url):
    return re.sub(r"(/jobs)/.*$", r"\1", url)  # `/jobs` ã®å¾Œã‚ã«ä½•ã‹ã‚ã‚Œã°å‰Šé™¤

# æœ€å¤§98ãƒšãƒ¼ã‚¸åˆ†ã®æ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆstart=0,10,20...970ï¼‰
for start in range(280, 290, 10):  # ä¾‹: 98ãƒšãƒ¼ã‚¸ãªã‚‰ range(0, 980, 10)
    print(f"ğŸ” {start//10+1}ãƒšãƒ¼ã‚¸ç›®ã‚’å–å¾—ä¸­...")

    # ãƒšãƒ¼ã‚¸ã”ã¨ã®æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    params = BASE_PARAMS.copy()
    params["start"] = start  # ãƒšãƒ¼ã‚¸ã®é–‹å§‹ä½ç½®

    # SerpApi ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    search = GoogleSearch(params)
    results = search.get_dict()

    # `organic_results` ã‹ã‚‰ `link` ã®ã¿ã‚’å–å¾—
    if "organic_results" in results:
        for result in results["organic_results"]:
            link = result.get("link", "")
            if link.startswith("https://hrmos.co/pages/"):
                cleaned_link = clean_url(link)  # æ•´å½¢å‡¦ç†
                job_urls.add(cleaned_link)  # setã«è¿½åŠ ï¼ˆé‡è¤‡ã‚’é™¤å¤–ï¼‰

    # 2ç§’ã®é…å»¶ï¼ˆAPIåˆ¶é™ã‚’è€ƒæ…®ï¼‰
    time.sleep(2)

# çµæœã‚’ `newURL.txt` ã«ä¿å­˜
with open("newURL.txt", "w", encoding="utf-8") as f:
    for url in sorted(job_urls):  # ã‚½ãƒ¼ãƒˆã—ã¦ä¿å­˜
        f.write(url + "\n")

print(f"âœ… å–å¾—å®Œäº†ï¼ {len(job_urls)} ä»¶ã®URLã‚’ `newURL.txt` ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

